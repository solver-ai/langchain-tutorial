{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "\n",
    "with open(\"../conf/service.dev.yaml\", 'r') as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "os.environ['OPENAI_API_KEY'] = configs['openai']['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chain = Prompt | Model | Parser\n",
    "\n",
    "## Grocery\n",
    "## 1. Prompt\n",
    "```\n",
    "langchain.prompts.ChatPromptTemplate.from_messages()\n",
    "langchain.prompts.ChatPromptTemplate.from_template()\n",
    "```\n",
    "\n",
    "## 2. Model\n",
    "```\n",
    "langchain_openai.ChatOpenAI()\n",
    "langchain_core.utils.function_calling.convert_to_openai_function()\n",
    "```\n",
    "\n",
    "## 3. Parser\n",
    "```\n",
    "langchain.schema.output_parser.StrOutputParser()\n",
    "langchain.agents.output_parsers.OpenAIFunctionsAgentOutputParser()\n",
    "from langchain.output_parsers.openai_functions.JsonOutputFunctionsParser()\n",
    "from langchain.output_parsers.openai_functions.JsonKeyOutputFunctionsParser()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic chain\n",
    "1. Prompt\n",
    "    - `from_template`\n",
    "2. Model\n",
    "    - `ChatOpenAI`\n",
    "3. Parser\n",
    "    - `StrOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({'topic': \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function calling\n",
    "1. Prompt\n",
    "    - `from_messages`\n",
    "2. Model\n",
    "    - `ChatOpenAI`\n",
    "    - `functions`\n",
    "3. Parser\n",
    "    - `OpenAIFunctionsAgentOutputParser`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define input class(`pydantic`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class WikipediaInput(BaseModel):\n",
    "    query: str = Field(description=\"Query to search\")\n",
    "    n_contents: int = Field(description=\"Number of contents to return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define python function(`tool`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "\n",
    "@tool(args_schema=WikipediaInput)\n",
    "def search_wikipedia(query: str, n_contents: int) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "\n",
    "    for title in titles[:n_contents]:\n",
    "        try:\n",
    "            page = wikipedia.page(title=title, auto_suggest=False)\n",
    "            summary = page.summary\n",
    "            summaries.append(f\"[{title}]\\n\"\n",
    "                             f\"{summary}\")\n",
    "        except (PageError, DisambiguationError):\n",
    "            pass\n",
    "    \n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    \n",
    "    return (50*\"=\").join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[LangChain]LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia({'query': \"langchain\", 'n_contents': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convert to OpenAI function(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'search_wikipedia',\n",
       "  'description': 'search_wikipedia(query: str, n_contents: int) -> str - Run Wikipedia search and get page summaries.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'description': 'Query to search',\n",
       "     'type': 'string'},\n",
       "    'n_contents': {'description': 'Number of contents to return',\n",
       "     'type': 'integer'}},\n",
       "   'required': ['query', 'n_contents']}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools = [search_wikipedia]\n",
    "# tools = [WikipediaInput]  # also possible\n",
    "functions = [convert_to_openai_function(tool) for tool in tools]\n",
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Bind function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "model = model.bind(\n",
    "    functions=functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Bind function + Forcing\n",
    "- 다른 방법들에 비해 `prompt_tokens` 개수가 가장 많으니 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "model = model.bind(\n",
    "    functions=functions,\n",
    "    function_call={'name': \"search_wikipedia\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Bind function + Forcing not to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "model = model.bind(\n",
    "    functions=functions,\n",
    "    function_call='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Build chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_values={'output': 'Hello! How can I assist you today?'} log='Hello! How can I assist you today?'\n",
      "tool='search_wikipedia' tool_input={'query': 'langchain', 'n_contents': 1} log=\"\\nInvoking: `search_wikipedia` with `{'query': 'langchain', 'n_contents': 1}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"langchain\",\\n  \"n_contents\": 1\\n}', 'name': 'search_wikipedia'}})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are helpful but sassy assistant\"),\n",
    "    ('user', \"{input}\")\n",
    "])\n",
    "parser = OpenAIFunctionsAgentOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(chain.invoke({'input': \"Hi!\"}))\n",
    "print(chain.invoke({'input': \"what is langchain?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example) Tagging & Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tagging\n",
    "텍스트로부터 구조화된 데이터를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentiment': 'pos', 'language': 'en'},\n",
       " {'sentiment': 'neg', 'language': 'it'},\n",
       " {'sentiment': 'neutral', 'language': 'ko'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of the text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\") \n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"Think carefully, and then tag the text as instructed\"),\n",
    "    ('user', \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI().bind(\n",
    "    functions=[convert_to_openai_function(Tagging)],\n",
    "    function_call={'name': \"Tagging\"}\n",
    ")\n",
    "parser = JsonOutputFunctionsParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.batch((\n",
    "    {'input': \"I love langchain\"},\n",
    "    {'input': \"non mi piace questo cibo\"},\n",
    "    {'input': \"이 식당은 밥을 적정량 준다\"}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Extraction\n",
    "Tagging과 비슷하지만 **다수**의 정보를 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': 0}]},\n",
       " {'people': [{'name': '민철', 'age': 30}, {'name': '희진', 'age': 50}]}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")\n",
    "\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: List[Person] = Field(description=\"List of info about people\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"Extract the relevant information, if not explicitly provided do not guess and stay empty. Extract partial info\"),\n",
    "    ('user', \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI().bind(\n",
    "    functions=[convert_to_openai_function(Information)],\n",
    "    function_call={'name': \"Information\"}\n",
    ")\n",
    "parser = JsonOutputFunctionsParser(key_name='people')\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.batch((\n",
    "    {'input': \"Joe is 30, his mom is Martha who is 20 years older than Joe\"},\n",
    "    {'input': \"민철이는 30살이고, 그보다 20살 많은 어머니의 이름은 희진입니다\"}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 값에 `20+30` 과 같이 수식이 들어간 경우, json parser가 인식하지 못한다.\n",
    "    - 이를 해결할 방법을 찾을 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = '{\\n \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"Martha\",\\n      \"age\": 30 + 20\\n    }\\n  ]\\n}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': 50}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Document extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "    \n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()\n",
    "doc = documents[0]\n",
    "doc.page_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'This document provides an overview of LLM-powered autonomous agents, including their components, such as planning, memory, and tool use. It discusses various techniques used in each component, such as task decomposition, self-reflection, and external API calls. The document also mentions proof-of-concept examples and challenges associated with building LLM-powered agents.',\n",
       " 'language': 'English',\n",
       " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, task decomposition, self-reflection, external API calls, proof-of-concept examples, challenges'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written.\")\n",
    "    keywords: str = Field(description=\"Provide keywords.\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are helpful summary expert for document. Extract the information from the given text.\"),\n",
    "    ('human', \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI().bind(\n",
    "    functions=[convert_to_openai_function(Overview)],\n",
    "    function_call={'name': \"Overview\"}\n",
    ")\n",
    "chain = prompt | model | JsonOutputFunctionsParser()\n",
    "\n",
    "chain.invoke({'input': doc.page_content[:10000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author': 'Wei et al. 2022'},\n",
       " {'author': 'Yao et al. 2023'},\n",
       " {'author': 'Liu et al. 2023'},\n",
       " {'author': 'Shinn & Labash 2023'},\n",
       " {'author': 'Laskin et al. 2023'},\n",
       " {'author': 'Duan et al. 2017'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str = Field(description=\"Title of the paper without authors\")\n",
    "    author: Optional[str] = Field(description=\"Author of the paper\")\n",
    "\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    papers: List[Paper] = Field(description=\"List of papers to extract from the text\")\n",
    "\n",
    "template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article.\n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # ('system', \"You are helpful paper extractor for the document. Think carefully then extract the information from the given text. Think step by step.\"),\n",
    "    ('system', template),\n",
    "    ('human', \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI(temperature=0).bind(\n",
    "    functions=[convert_to_openai_function(Information)],\n",
    "    function_call={'name': \"Information\"}\n",
    ")\n",
    "chain = prompt | model | JsonKeyOutputFunctionsParser(key_name='papers')\n",
    "\n",
    "chain.invoke({'input': doc.page_content[:10000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `title`은 required 인데도 잘 안 되는 경우가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Information',\n",
       " 'description': 'Information to extract.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'papers': {'description': 'List of papers to extract from the text',\n",
       "    'type': 'array',\n",
       "    'items': {'description': 'Information about papers mentioned.',\n",
       "     'type': 'object',\n",
       "     'properties': {'author': {'description': 'Author of the paper',\n",
       "       'type': 'string'}},\n",
       "     'required': ['title']}}},\n",
       "  'required': ['papers']}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_function(Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 긴 문서는 잘라서 넣고 나중에 합친다.\n",
    "    - `langchain.text_splitter.RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author': 'Lilian Weng'},\n",
       " {'author': 'Wei et al. 2022'},\n",
       " {'author': 'Yao et al. 2023'},\n",
       " {'author': 'Liu et al. 2023'},\n",
       " {'author': 'Shinn & Labash 2023'},\n",
       " {'author': 'Shinn & Labash, 2023'},\n",
       " {'author': 'Liu et al. 2023'},\n",
       " {'author': 'Laskin et al. 2023'},\n",
       " {'author': 'Laskin et al. 2023'},\n",
       " {'author': 'Duan et al. 2017'},\n",
       " {'author': 'Miller 1956'},\n",
       " {'author': 'LSH (Locality-Sensitive Hashing)'},\n",
       " {'author': 'ANNOY (Approximate Nearest Neighbors Oh Yeah)'},\n",
       " {'author': 'HNSW (Hierarchical Navigable Small World)'},\n",
       " {'author': 'FAISS (Facebook AI Similarity Search)'},\n",
       " {'author': 'ScaNN (Scalable Nearest Neighbors)'},\n",
       " {'author': 'Karpas et al. 2022'},\n",
       " {'author': 'Parisi et al. 2022'},\n",
       " {'author': 'Schick et al. 2023'},\n",
       " {'author': 'Shen et al. 2023'},\n",
       " {'author': 'Li et al. 2023'},\n",
       " {'author': 'Bran et al. 2023'},\n",
       " {'author': 'Boiko et al. (2023)'},\n",
       " {'author': 'Park, et al. (2023)'},\n",
       " {'author': 'Park et al.'},\n",
       " {'author': 'John Smith'},\n",
       " {'author': 'Jane Doe'},\n",
       " {'author': 'John Doe'},\n",
       " {'author': 'Jane Smith'},\n",
       " {'author': 'Wei et al.'},\n",
       " {'author': 'Yao et al.'},\n",
       " {'author': 'Liu et al.'},\n",
       " {'author': 'Google Blog'},\n",
       " {'author': 'Shinn & Labash'},\n",
       " {'author': 'Laskin et al.'},\n",
       " {'author': 'Karpas et al.'},\n",
       " {'author': 'Li et al.'},\n",
       " {'author': 'Shen et al.'},\n",
       " {'author': 'Bran et al.'},\n",
       " {'author': 'Boiko et al.'},\n",
       " {'author': 'Joon Sung Park, et al.'},\n",
       " {'author': 'AutoGPT'},\n",
       " {'author': 'GPT-Engineer'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\n",
    "splits = text_splitter.split_text(doc.page_content)\n",
    "prep = RunnableLambda(lambda x: [{'input': doc} for doc in text_splitter.split_text(x)])\n",
    "split_chain = prep | chain.map() | flatten\n",
    "\n",
    "split_chain.invoke(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fallback handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `json.loads` 가 정확한  json format을 입력받지 못해 에러가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 9 column 1 (char 125)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m main_chain \u001b[38;5;241m=\u001b[39m main_model \u001b[38;5;241m|\u001b[39m json\u001b[38;5;241m.\u001b[39mloads\n\u001b[1;32m     12\u001b[0m challenge \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite three poems in a json blob, where each poem is a json blob of a title, author, and first line\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmain_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchallenge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/runnables/base.py:2053\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2053\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/runnables/base.py:3507\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3515\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3517\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/runnables/base.py:1246\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1243\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1244\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1245\u001b[0m         Output,\n\u001b[0;32m-> 1246\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/runnables/base.py:3383\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3381\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3383\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3385\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3386\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 9 column 1 (char 125)"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "main_model = OpenAI(\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    "    model='gpt-3.5-turbo-instruct'\n",
    ")\n",
    "main_chain = main_model | json.loads\n",
    "\n",
    "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
    "main_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 최신 LLM 사용방식은 정확한 json format을 줄 수 있음\n",
    "    1. `langchain.llms.OpenAI`의 output은 `str`이고, \\\n",
    "    `langchain_openai.ChatOpenAI`의 output은 `AIMessage` 객채\n",
    "    2. Output의 format을 `str`로 맞춰주기 위해 `StrOutputParser` 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'In the stillness of night, whispers of the wind'},\n",
       " 'poem2': {'title': 'Silent Tears',\n",
       "  'author': 'Jacob Anderson',\n",
       "  'first_line': 'Silent tears fall, unseen and unheard'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Lee',\n",
       "  'first_line': 'Beneath the moonlight, dancing shadows sway'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import json\n",
    "\n",
    "\n",
    "fallback_model = ChatOpenAI(temperature=0)\n",
    "fallback_chain = fallback_model | StrOutputParser() | json.loads\n",
    "fallback_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 정상작동하는 2번 방법을 fallback method로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
       " 'poem2': {'title': 'Silent Serenade',\n",
       "  'author': 'Jacob Moore',\n",
       "  'first_line': 'In the stillness of night, a silent serenade'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Anderson',\n",
       "  'first_line': 'Shadows dance upon the moonlit floor'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = main_chain.with_fallbacks([fallback_chain])\n",
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Batch processing\n",
    "가능한 batch 처리되는 프로세스들은 병렬적으로 실행된다. (default `n_procs=5`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n",
       " 'Why don\\'t frogs make good loan officers?\\n\\nBecause they always end up in \"debt\"!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.batch([\n",
    "    {'topic': \"bears\"},\n",
    "    {'topic': \"frogs\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Streaming processing\n",
    "적은 지연시간은 더 나은 사용자 경험을 만들어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " don\n",
      "'t\n",
      " bears\n",
      " wear\n",
      " shoes\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " already\n",
      " have\n",
      " bear\n",
      " feet\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "for t in chain.stream({'topic': \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Asynchronous methods\n",
    "용도에 따라 비동기적 처리도 가능 (`ainvoke`, `abatch`, `astream`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = await chain.ainvoke({'topic': \"bears\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Routing\n",
    "사용되는 tool에 따라 다른 방식으로 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "class PositiveInput(BaseModel):\n",
    "    persons: List[str] = Field(description=\"Name list of persons having positive action\")\n",
    "\n",
    "@tool(args_schema=PositiveInput)\n",
    "def positive(persons: List[str]) -> str:\n",
    "    \"\"\"Handle persons having only positive action.\"\"\"\n",
    "    return f\"[Positive] {persons}\"\n",
    "\n",
    "class NegativeInput(BaseModel):\n",
    "    persons: List[str] = Field(description=\"Name list of persons having negative action\")\n",
    "\n",
    "@tool(args_schema=NegativeInput)\n",
    "def negative(persons: List[str]) -> str:\n",
    "    \"\"\"Handle persons having only negative action.\"\"\"\n",
    "    return f\"[Negative] {persons}\"\n",
    "\n",
    "class NeutralInput(BaseModel):\n",
    "    persons: List[str] = Field(description=\"Name list of persons having negative action\")\n",
    "\n",
    "@tool(args_schema=NeutralInput)\n",
    "def neutral(persons: List[str]) -> str:\n",
    "    \"\"\"Handle persons having only neutral action.\"\"\"\n",
    "    return f\"[Neutral] {persons}\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are expert of discrimating persons with emotional action. Positive or negative or neutral.\"),\n",
    "    ('human', \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI(temperature=0).bind(\n",
    "    functions=[convert_to_openai_function(fn) for fn in [positive, negative, neutral]]\n",
    ")\n",
    "parser = OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.agents.AgentActionMessageLog,\n",
       " 'positive',\n",
       " {'persons': ['Jimmy', 'Sansu']})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = chain.invoke({'input': \"Jimmy like to eat vegatables and Sansu wants to listen piano.\"})\n",
    "\n",
    "# result = chain.batch([\n",
    "#     {'input': \"Jimmy like to eat vegatables and Sansu wants to listen piano.\"},\n",
    "#     {'input': \"Jimmy does not eat vegatables and Sansu hates to listen piano.\"},\n",
    "#     {'input': \"Jimmy looks at Sansu.\"}\n",
    "# ])\n",
    "type(result1), result1.tool, result1.tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.agents.AgentFinish,\n",
       " {'output': \"Hello! I'm an AI assistant, so I don't have feelings, but I'm here to help you. How can I assist you today?\"})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = chain.invoke({'input': \"Hi, how are you?\"})\n",
    "type(result2), result2.return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        return eval(result.tool).run(result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser | route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[Positive] ['Jimmy', 'Sansu']\",\n",
       " \"Hello! I'm an AI assistant, so I don't have feelings, but I'm here to help you. How can I assist you today?\"]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\n",
    "    {'input': \"Jimmy like to eat vegatables and Sansu wants to listen piano.\"},\n",
    "    {'input': \"Hi, how are you?\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 문장 안에서 여러 개의 tool이 사용되는 경우는 처리하지 못하는 모양"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Positive] ['Hammy']\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"\"\"\n",
    "              Jimmy does not eat vegatables but Hammy likes to eat vegatables.\n",
    "              Minsu likes to play piano but Sansu hates to listen it.\n",
    "              \"\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ChatBot\n",
    "Multi-turn 대화를 위해 `placeholder`, `memory`를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 감정 분석 chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import tool\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "class WikipediaInput(BaseModel):\n",
    "    query: str = Field(description=\"Query to search\")\n",
    "    n_contents: int = Field(description=\"Number of contents to return\")\n",
    "\n",
    "@tool(args_schema=WikipediaInput)\n",
    "def search_wikipedia(query: str, n_contents: int) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "\n",
    "    for title in titles[:n_contents]:\n",
    "        try:\n",
    "            page = wikipedia.page(title=title, auto_suggest=False)\n",
    "            summary = page.summary\n",
    "            summaries.append(f\"[{title}]\\n\"\n",
    "                             f\"{summary}\")\n",
    "        except (PageError, DisambiguationError):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return (50*\"=\").join(summaries)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are helpful but sassy assistant\"),\n",
    "    ('user', \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI(temperature=0).bind(\n",
    "    functions=[convert_to_openai_function(search_wikipedia)]\n",
    ")\n",
    "parser = OpenAIFunctionsAgentOutputParser()\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'Hello! How can I assist you today?'}, log='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"Hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='search_wikipedia', tool_input={'query': 'Langchain', 'n_contents': 1}, log=\"\\nInvoking: `search_wikipedia` with `{'query': 'Langchain', 'n_contents': 1}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Langchain\",\\n  \"n_contents\": 1\\n}', 'name': 'search_wikipedia'}})])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import MessagePlaceholder\n",
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "def run_agent(user_input):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
